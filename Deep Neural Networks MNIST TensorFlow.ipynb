{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network for MNIST Classification\n",
    "The problem we've chosen is referred to as the \"Hello World\" for machine learning. The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional networks. The dataset provides 28x28 images of handwritten digits (1 per image) and the goal is to write an algorithm that detects which digit is written. Since there are only 10 digits, this is a classification problem with 10 classes. In order to exemplify what we've talked about in this section, we will build a network with 2 hidden layers between inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-dae31dc7d5d5>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/lucaszarzeczny/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/lucaszarzeczny/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/lucaszarzeczny/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/lucaszarzeczny/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/lucaszarzeczny/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# data has been split into training, validation, and test\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 5000\n",
    "\n",
    "# Reset any variables left in memory from previous runs.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# As in the previous example - declare placeholders where the data will be fed into.\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "# Weights and biases for the first linear combination between the inputs and the first hidden layer.\n",
    "# Use get_variable in order to make use of the default TensorFlow initializer which is Xavier.\n",
    "weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
    "biases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\n",
    "\n",
    "# Operation between the inputs and the first hidden layer.\n",
    "# We've chosen ReLu as our activation function. You can try playing with different non-linearities.\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "# Weights and biases for the second linear combination.\n",
    "# This is between the first and second hidden layers.\n",
    "weights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "\n",
    "# Operation between the first and the second hidden layers. Again, we use ReLu.\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    "\n",
    "# Don't forget to change the shape of weights_3 and biases_3. \n",
    "# They used to lead to the output layer, but now they lead to the third hidden layer\n",
    "weights_3 = tf.get_variable(\"weights_3\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_3 = tf.get_variable(\"biases_3\", [hidden_layer_size])\n",
    "\n",
    "# Create outputs_3 variable. I'll use ReLu once again\n",
    "outputs_3 = tf.nn.relu(tf.matmul(outputs_2,weights_3) + biases_3)\n",
    "\n",
    "weights_4 = tf.get_variable(\"weights_4\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_4 = tf.get_variable(\"biases_4\", [hidden_layer_size])\n",
    "outputs_4 = tf.nn.relu(tf.matmul(outputs_3,weights_4) + biases_4)\n",
    "\n",
    "weights_5 = tf.get_variable(\"weights_5\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_5 = tf.get_variable(\"biases_5\", [hidden_layer_size])\n",
    "outputs_5 = tf.nn.relu(tf.matmul(outputs_4,weights_5) + biases_5)\n",
    "\n",
    "weights_6 = tf.get_variable(\"weights_6\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_6 = tf.get_variable(\"biases_6\", [hidden_layer_size])\n",
    "outputs_6 = tf.nn.relu(tf.matmul(outputs_5,weights_6) + biases_6)\n",
    "\n",
    "weights_7 = tf.get_variable(\"weights_7\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_7 = tf.get_variable(\"biases_7\", [hidden_layer_size])\n",
    "outputs_7 = tf.nn.relu(tf.matmul(outputs_6,weights_7) + biases_7)\n",
    "\n",
    "weights_8 = tf.get_variable(\"weights_8\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_8 = tf.get_variable(\"biases_8\", [hidden_layer_size])\n",
    "outputs_8 = tf.nn.relu(tf.matmul(outputs_7,weights_8) + biases_8)\n",
    "\n",
    "weights_9 = tf.get_variable(\"weights_9\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_9 = tf.get_variable(\"biases_9\", [hidden_layer_size])\n",
    "outputs_9 = tf.nn.relu(tf.matmul(outputs_8,weights_9) + biases_9)\n",
    "\n",
    "weights_10 = tf.get_variable(\"weights_10\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_10 = tf.get_variable(\"biases_10\", [hidden_layer_size])\n",
    "outputs_10 = tf.nn.relu(tf.matmul(outputs_9,weights_10) + biases_10)\n",
    "\n",
    "weights_11 = tf.get_variable(\"weights_11\", [hidden_layer_size, output_size])\n",
    "biases_11 = tf.get_variable(\"biases_11\", [output_size])\n",
    "\n",
    "# The outputs are a function of outputs_4, weights_5, and biases_5\n",
    "outputs = tf.matmul(outputs_10, weights_11) + biases_11\n",
    "\n",
    "\n",
    "# Calculate the loss function for every output/target pair.\n",
    "# The function used is the same as applying softmax to the last layer and then calculating cross entropy\n",
    "# with the function we've seen in the lectures. This function, however, combines them in a clever way, \n",
    "# which makes it both faster and more numerically stable (when dealing with very small numbers).\n",
    "# Logits here means: unscaled probabilities (so, the outputs, before they are scaled by the softmax)\n",
    "# Naturally, the labels are the targets.\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, labels=targets)\n",
    "\n",
    "# Get the average loss\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "# Define the optimization step. Using adaptive optimizers such as Adam in TensorFlow\n",
    "# is as simple as that.\n",
    "optimize = tf.train.AdamOptimizer(learning_rate=0.0002).minimize(mean_loss)\n",
    "\n",
    "# Get a 0 or 1 for every input in the batch indicating whether it output the correct answer out of the 10.\n",
    "out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "\n",
    "# Get the average accuracy of the outputs.\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
    "\n",
    "# Declare the session variable.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Initialize the variables. Default initializer is Xavier.\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "# Batching\n",
    "batch_size = 150\n",
    "\n",
    "# Calculate the number of batches per epoch for the training set.\n",
    "batches_number = mnist.train._num_examples // batch_size\n",
    "\n",
    "# Basic early stopping. Set a miximum number of epochs.\n",
    "max_epochs = 50\n",
    "\n",
    "# Keep track of the validation loss of the previous epoch.\n",
    "# If the validation loss becomes increasing, we want to trigger early stopping.\n",
    "# We initially set it at some arbitrarily high number to make sure we don't trigger it\n",
    "# at the first epoch\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a loop for the epochs. Epoch_counter is a variable which automatically starts from 0.\n",
    "for epoch_counter in range(max_epochs):\n",
    "    \n",
    "    # Keep track of the sum of batch losses in the epoch.\n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    # Iterate over the batches in this epoch.\n",
    "    for batch_counter in range(batches_number):\n",
    "        \n",
    "        # Input batch and target batch are assigned values from the train dataset, given a batch size\n",
    "        input_batch, target_batch = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # Run the optimization step and get the mean loss for this batch.\n",
    "        # Feed it with the inputs and the targets we just got from the train dataset\n",
    "        _, batch_loss = sess.run([optimize, mean_loss], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "        # Increment the sum of batch losses.\n",
    "        curr_epoch_loss += batch_loss\n",
    "    \n",
    "    # So far curr_epoch_loss contained the sum of all batches inside the epoch\n",
    "    # We want to find the average batch losses over the whole epoch\n",
    "    # The average batch loss is a good proxy for the current epoch loss\n",
    "    curr_epoch_loss /= batches_number\n",
    "    \n",
    "    # At the end of each epoch, get the validation loss and accuracy\n",
    "    # Get the input batch and the target batch from the validation dataset\n",
    "    input_batch, target_batch = mnist.validation.next_batch(mnist.validation._num_examples)\n",
    "    \n",
    "    # Run without the optimization step (simply forward propagate)\n",
    "    validation_loss, validation_accuracy = sess.run([mean_loss, accuracy], \n",
    "        feed_dict={inputs: input_batch, targets: target_batch})\n",
    "    \n",
    "    # Print statistics for the current epoch\n",
    "    # Epoch counter + 1, because epoch_counter automatically starts from 0, instead of 1\n",
    "    # We format the losses with 3 digits after the dot\n",
    "    # We format the accuracy in percentages for easier interpretation\n",
    "    print('Epoch '+str(epoch_counter+1)+\n",
    "          '. Mean loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "          '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
    "    \n",
    "    # Trigger early stopping if validation loss begins increasing.\n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "        \n",
    "    # Store this epoch's validation loss to be used as previous validation loss in the next iteration.\n",
    "    prev_validation_loss = validation_loss\n",
    "\n",
    "# Not essential, but it is nice to know when the algorithm stopped working in the output section, rather than check the kernel\n",
    "print('End of training.')\n",
    "\n",
    "#Add the time it took the algorithm to train\n",
    "print(\"Training time: %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final accuracy of the model comes from forward propagating through the test dataset as otherwise we may overfit\n",
    "# It took 5 epochs until the validation loss started increasing\n",
    "# Batching much faster\n",
    "# Validation helps eliminate overfitting (the parameters W and B)\n",
    "# All can be adjusted\n",
    " # Width, depth, Activations, learning rate, and batch size\n",
    "# We try to find the best hyperparameters, but not the best parameters, they fit the validation set the best\n",
    "# Test data set is the set the machine has not seen yet\n",
    "# Validation is not the best instance for accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, target_batch = mnist.test.next_batch(mnist.test.num_examples)\n",
    "test_accuracy = sess.run([accuracy],\n",
    "                        feed_dict = {inputs: input_batch, targets: target_batch})\n",
    "\n",
    "test_accuracy_percent = test_accuracy[0]*100.\n",
    "\n",
    "print('Test accuracy: '+'{0:2f}'.format(test_accuracy_percent)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The test accuracy is the final accuracy of the model\n",
    "# Gets 97 out of 98 digits right!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "There are several main adjustments you may try.\n",
    "\n",
    "It is useful to take note of the time it takes the algorithm to train. This will provide you with an additional level of understanding. That is also your first task.\n",
    "\n",
    "Using the code from the lecture as the basis, fiddle with the hyperparameters of the algorithm.\n",
    "\n",
    "The width (the hidden layer size) of the algorithm. Try a hidden layer size of 200. How does the validation accuracy of the model change? What about the time it took the algorithm to train? Can you find a hidden layer size that does better?\n",
    "    # Updating the hidden layer size to 200 has a minimal effect. \n",
    "\n",
    "The depth of the algorithm. Add another hidden layer to the algorithm. This is an extremely important exercise! How does the validation accuracy change? What about the time it took the algorithm to train? Hint: Be careful with the shapes of the weights and the biases.\n",
    "\n",
    "    # Adding another hidden layer actually reduced accuracy slightly\n",
    "\n",
    "The width and depth of the algorithm. Add as many additional layers as you need to reach 5 hidden layers. Moreover, adjust the width of the algorithm as you find suitable. How does the validation accuracy change? What about the time it took the algorithm to train?\n",
    "\n",
    "    # Adding multiple hidden layers and increasing or decreasing the width actually reduced accuracy slightly\n",
    "\n",
    "Fiddle with the activation functions. Try applying sigmoid transformation to both layers. The sigmoid activation is given by the method: tf.nn.sigmoid()\n",
    "\n",
    "    # Get 95.62 % Accuracy - decreases\n",
    "\n",
    "Fiddle with the activation functions. Try applying a ReLu to the first hidden layer and tanh to the second one. The tanh activation is given by the method: tf.nn.tanh()\n",
    "\n",
    "    #  All tanh get an accuracy of 96.98\n",
    "\n",
    "Adjust the batch size. Try a batch size of 1000. How does the required time change? What about the accuracy?\n",
    "\n",
    "    # Increasing the batch size takes the result to 97.229999%\n",
    "\n",
    "Adjust the batch size. Try a batch size of 1. That's the SGD. How do the time and accuracy change? Is the result coherent with the theory?\n",
    "\n",
    "    # Time increases drastically due because the batch size is so small. It is \"crawling\" Accuracy is slightly lower.\n",
    "\n",
    "Adjust the learning rate. Try a value of 0.0001. Does it make a difference?\n",
    "\n",
    "    # Accuracy drops to 93.5%\n",
    "\n",
    "Adjust the learning rate. Try a value of 0.02. Does it make a difference?\n",
    " \n",
    "    # Learning rate 94.58%\n",
    "\n",
    "Combine all the methods above and try to reach a validation accuracy of 98.5+ percent.\n",
    "\n",
    "    #hidden_layer_size = 5000\n",
    "    #batch_size = 150\n",
    "    #learning_rate = 0.0002\n",
    "    #Hidden Layers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
